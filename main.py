import yaml
import torch
import argparse
import numpy as np
import os
from pathlib import Path
from sklearn.metrics import f1_score, accuracy_score
from transformers import (
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    set_seed,
    EarlyStoppingCallback
)
# ç»Ÿä¸€ä½¿ç”¨ä½ æœ€æ–°çš„å¤šä»»åŠ¡æ•°æ®é›†ç±»
from src.dataset.multitask_data_loader import MultitaskPolarDataset


def compute_metrics(eval_pred):
    """é€šç”¨äºŒåˆ†ç±»æŒ‡æ ‡è®¡ç®—"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "f1_macro": f1_score(labels, predictions, average='macro'),
        "accuracy": accuracy_score(labels, predictions)
    }


def main():
    # --- 1. å‘½ä»¤è¡Œå‚æ•°è§£æ ---
    parser = argparse.ArgumentParser(description="POLAR SemEval 2026 Training Entry")
    parser.add_argument("--config", type=str, required=True,
                        help="Path to the config file (e.g., configs/augmented_st1.yaml)")
    parser.add_argument("--task", type=str, default="st1", help="Task name: st1, st2, or st3")
    args = parser.parse_args()

    # --- 2. åŠ è½½æŒ‡å®šé…ç½® ---
    if not os.path.exists(args.config):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {args.config}")

    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # è®¾ç½®éšæœºç§å­
    set_seed(config['train'].get('seed', 42))
    print(f"ğŸš€ å·²åŠ è½½é…ç½®: {args.config} | ä»»åŠ¡: {args.task}")

    # --- 3. æ„é€ æ•°æ®é›† ---
    # è¿™é‡Œç»Ÿä¸€ä½¿ç”¨ MultitaskPolarDatasetï¼Œé€šè¿‡ args.task åˆ‡æ¢ä»»åŠ¡
    full_dataset = MultitaskPolarDataset(
        data_path=config['data']['train_file'],
        tokenizer_name=config['model']['backbone'],
        max_length=config['model']['max_length'],
        task=args.task
    )

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int((1 - config['data']['val_split']) * len(full_dataset))
    val_size = len(full_dataset) - train_size

    train_ds, val_ds = torch.utils.data.random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(config['train'].get('seed', 42))
    )
    print(f"ğŸ“Š æ•°æ®å°±ç»ª: è®­ç»ƒé›† {len(train_ds)}, éªŒè¯é›† {len(val_ds)}")

    # --- 4. åŠ è½½æ¨¡å‹ ---
    model = AutoModelForSequenceClassification.from_pretrained(
        config['model']['backbone'],
        num_labels=config['model'].get('num_labels', 2)
    )

    # --- 5. å®šä¹‰è®­ç»ƒå‚æ•° ---
    # è¿™é‡Œçš„ output_dir ä¼šæ ¹æ®é…ç½®æ–‡ä»¶è‡ªåŠ¨åˆ‡æ¢è·¯å¾„
    training_args = TrainingArguments(
        output_dir=config['train']['output_dir'],
        num_train_epochs=config['train']['epochs'],
        per_device_train_batch_size=config['train']['batch_size'],
        per_device_eval_batch_size=config['train']['batch_size'],
        learning_rate=float(config['train']['learning_rate']),

        # ç­–ç•¥è®¾ç½®
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        save_total_limit=2,  # é™åˆ¶ä¿å­˜æ•°é‡ï¼Œé˜²æ­¢ç£ç›˜æ»¡

        # ç¡¬ä»¶ä¼˜åŒ–
        fp16=torch.cuda.is_available(),
        report_to="none",
        logging_dir="./logs"
    )

    # --- 6. å®ä¾‹åŒ– Trainer ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # å¢åŠ æ—©åœä¿æŠ¤
    )

    # --- 7. æ‰§è¡Œè®­ç»ƒ ---
    print(f"ğŸ”¥ æ­£åœ¨å¯åŠ¨è®­ç»ƒï¼Œè¾“å‡ºç›®å½•: {config['train']['output_dir']}")
    trainer.train()

    # --- 8. æœ€ç»ˆä¿å­˜ ---
    final_save_path = Path(config['train']['output_dir']) / "final_model"
    trainer.save_model(final_save_path)
    full_dataset.tokenizer.save_pretrained(final_save_path)
    print(f"âœ… ä»»åŠ¡å®Œæˆï¼æƒé‡å·²å¯¼å‡ºè‡³: {final_save_path}")


if __name__ == "__main__":
    main()