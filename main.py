import os
import yaml
import argparse
import torch
from pathlib import Path
from transformers import TrainingArguments, set_seed, EarlyStoppingCallback, AutoTokenizer

# 1. å¯¼å…¥å·²æ‹†åˆ†å¹¶ä¼˜åŒ–çš„æ¨¡å—
from src.dataset.polar_dataset import MultitaskPolarDataset
from src.dataset.data_collator import get_polar_collator
from src.models.backbone import XLMRobertaForPolarization
from src.engine.trainer import FGMTrainer
from src.engine.evaluator import compute_metrics


def parse_args():
    parser = argparse.ArgumentParser(description="POLAR SemEval 2026 Training Pipeline")
    parser.add_argument("--config", type=str, required=True, help="YAML é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--task", type=str, default="st1", choices=["st1", "st2", "st3"], help="å­ä»»åŠ¡åç§°")
    return parser.parse_args()


def main():
    # --- é˜¶æ®µ A: ç¯å¢ƒä¸é…ç½®å‡†å¤‡ ---
    args = parse_args()
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # è®¾ç½®éšæœºç§å­ä¿è¯å®éªŒå¯å¤ç°
    set_seed(config['train'].get('seed', 42))

    # ç¡®å®šè¾“å‡ºç›®å½•å¹¶è‡ªåŠ¨åˆ›å»º
    output_dir = Path(config['train']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    # --- é˜¶æ®µ B: æ•°æ®æµæ°´çº¿ (Dataset + Collator) ---
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®ä»»åŠ¡: {args.task.upper()}")

    # å®ä¾‹åŒ–æ”¯æŒæ¨ç†æ³¨å…¥å’Œ K-Fold çš„ Dataset
    full_dataset = MultitaskPolarDataset(
        data_path=config['data']['train_file'],
        tokenizer_name=config['model']['backbone'],
        max_length=config['model'].get('max_length', 256),
        task=args.task
    )

    # éªŒè¯é›†åˆ‡åˆ†
    val_split = config['data'].get('val_split', 0.1)
    train_size = int((1 - val_split) * len(full_dataset))
    train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, len(full_dataset) - train_size])

    # ğŸš€ æ”¹è¿› 1: å¼•å…¥åŠ¨æ€å¡«å…… (Dynamic Padding)
    # ä¸å†åœ¨ Dataset é‡Œå¼ºåˆ¶ padding åˆ°æœ€å¤§é•¿åº¦ï¼Œè€Œæ˜¯æŒ‰ Batch åŠ¨æ€å¯¹é½
    tokenizer = AutoTokenizer.from_pretrained(config['model']['backbone'])
    data_collator = get_polar_collator(tokenizer)

    # --- é˜¶æ®µ C: æ¨¡å‹ç»„è£… (Backbone + Multi-Sample Dropout) ---
    # ğŸš€ æ”¹è¿› 2: æ¥å…¥è‡ªå®šä¹‰æ¨¡å‹åº•åº§
    # ä½¿ç”¨é›†æˆ 5 ç»„å¹¶è¡Œ Dropout çš„å®šåˆ¶æ¨¡å‹ï¼Œè€ŒéåŸç”Ÿåˆ†ç±»æ¨¡å‹
    print(f"ğŸ§  æ­£åœ¨ç»„è£…è‡ªå®šä¹‰æ¨¡å‹ (åº•åº§: {config['model']['backbone']})")
    model = XLMRobertaForPolarization(
        model_name=config['model']['backbone'],
        num_labels=config['model'].get('num_labels', 2),
        use_multi_dropout=config['model'].get('use_multi_dropout', True),
        num_dropout=config['model'].get('num_dropout', 5)
    )

    # --- é˜¶æ®µ D: è®­ç»ƒå‚æ•°ä¸å¼•æ“è®¾å®š ---
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=config['train']['epochs'],
        per_device_train_batch_size=config['train']['batch_size'],
        per_device_eval_batch_size=config['train']['batch_size'],
        learning_rate=float(config['train']['learning_rate']),
        weight_decay=config['train'].get('weight_decay', 0.01),
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        fp16=torch.cuda.is_available(),  # è‡ªåŠ¨æ£€æµ‹å¹¶å¼€å¯æ··åˆç²¾åº¦
        save_total_limit=2,  # ä»…ä¿ç•™æœ€è¿‘ 2 ä¸ªæ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœç£ç›˜
        report_to="none"
    )

    # ğŸš€ æ”¹è¿› 3: é…ç½® FGM å¯¹æŠ—è®­ç»ƒå¼€å…³
    # åªæœ‰å½“ YAML ä¸­è®¾ç½® use_fgm ä¸º True æ—¶æ‰æ¿€æ´»æ‰°åŠ¨
    use_fgm = config['train'].get('use_fgm', True)
    fgm_eps = config['train'].get('fgm_eps', 0.5) if use_fgm else 0.0

    trainer = FGMTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        data_collator=data_collator,  # ä¼ å…¥åŠ¨æ€å¡«å……å™¨
        compute_metrics=compute_metrics,  # ç»Ÿä¸€è¯„ä»·æŒ‡æ ‡
        fgm_epsilon=fgm_eps,  # å¯¹æŠ—æ‰°åŠ¨ç³»æ•°
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    # --- é˜¶æ®µ E: æ‰§è¡Œè®­ç»ƒ ---
    print(f"ğŸ”¥ è®­ç»ƒå¯åŠ¨ï¼å¯¹æŠ—æ‰°åŠ¨: {'å¼€å¯ (eps=' + str(fgm_eps) + ')' if use_fgm else 'å…³é—­'}")
    trainer.train()

    # æœ€ç»ˆæ¨¡å‹ä¿å­˜ï¼Œç¡®ä¿ get_outputs.py å¯ä»¥ç›´æ¥è¯»å–
    final_save_path = output_dir / "final_model"
    trainer.save_model(final_save_path)
    tokenizer.save_pretrained(final_save_path)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³: {final_save_path}")


if __name__ == "__main__":
    main()