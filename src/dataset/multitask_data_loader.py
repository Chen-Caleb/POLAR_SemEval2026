import json
import torch
import os
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from pathlib import Path


class MultitaskPolarDataset(Dataset):
    """
    æ›´æ–°ç‰ˆï¼šæ”¯æŒ Augmented æ•°æ®çš„æ¨ç†æ³¨å…¥ä¸å¤šä»»åŠ¡æ ‡ç­¾æå–
    - ST1: label_st1 (int)
    - ST2: label_st2 (list of 5)
    - ST3: label_st3 (list of 6)
    """

    def __init__(self, data_path, tokenizer_name, max_length=256, task="st1"):
        self.data = []
        self.task = task.lower()
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length

        # --- 1. æ™ºèƒ½è·¯å¾„è§£æ ---
        # å…¼å®¹ Colab ç›¸å¯¹è·¯å¾„ä¸é¡¹ç›®ç»“æ„
        current_path = Path(os.getcwd())
        absolute_path = current_path / data_path if not Path(data_path).is_absolute() else Path(data_path)

        print(f"ğŸ” æ­£åœ¨åŠ è½½æ•°æ®: {absolute_path}")

        with open(absolute_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip(): continue
                item = json.loads(line)

                # æå–å…±æœ‰å­—æ®µ
                text = item["text"]
                analysis = item.get("analysis")  # å¯èƒ½ä¸º None

                # --- 2. æ ‡ç­¾æå–é€»è¾‘ (é€‚é…æœ€æ–° JSON æ ¼å¼) ---
                if self.task == "st1":
                    # ç›´æ¥è·å– label_st1 (0 æˆ– 1)
                    label = item.get("label_st1")
                    if label is not None and label != -1:
                        self.data.append({"text": text, "label": label, "analysis": analysis})

                elif self.task == "st2":
                    # ç›´æ¥è·å– label_st2 åˆ—è¡¨ [1, 1, 0, 0, 0]
                    label = item.get("label_st2")
                    if label and item.get("label_st1") == 1:
                        self.data.append({"text": text, "label": label, "analysis": analysis})

                elif self.task == "st3":
                    # ç›´æ¥è·å– label_st3 åˆ—è¡¨ [1, 1, 0, 1, 1, 0]
                    label = item.get("label_st3")
                    if label and item.get("label_st1") == 1:
                        self.data.append({"text": text, "label": label, "analysis": analysis})

        print(f"âœ… {self.task.upper()} åŠ è½½å®Œæˆï¼æ ·æœ¬æ•°: {len(self.data)}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item["text"]
        analysis = item["analysis"]

        # --- 3. æ¨ç†æ³¨å…¥ (Reasoning Injection) ---
        # å¦‚æœæœ‰åˆ†æä¿¡æ¯ï¼Œåˆ©ç”¨æ¨¡å‹çš„ sep_token è¿›è¡Œæ‹¼æ¥
        if analysis:
            sep = self.tokenizer.sep_token
            # æ‹¼æ¥æ ¼å¼: [Text] </s> Analysis: [Reasoning]
            text = f"{text} {sep} Analysis: {analysis}"

        # æ–‡æœ¬åˆ†è¯
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # --- 4. æ ‡ç­¾ç±»å‹è½¬æ¢ ---
        if self.task == "st1":
            # å•æ ‡ç­¾äºŒåˆ†ç±»ç”¨ Long
            label_tensor = torch.tensor(item["label"], dtype=torch.long)
        else:
            # å¤šæ ‡ç­¾ä»»åŠ¡ç”¨ Float (å¯¹åº” BCEWithLogitsLoss)
            label_tensor = torch.tensor(item["label"], dtype=torch.float)

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": label_tensor
        }