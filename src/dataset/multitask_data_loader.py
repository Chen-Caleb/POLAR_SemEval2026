import json
import torch
import os
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from pathlib import Path


class MultitaskPolarDataset(Dataset):
    """
    SemEval 2026 POLAR ä»»åŠ¡é€šç”¨æ•°æ®é›†ç±»
    æ”¯æŒ:
    - ST1: äºŒåˆ†ç±» (Polarized vs Non-polarized)
    - ST2: å¤šæ ‡ç­¾ç»´åº¦è¯†åˆ« (Political, Religious, etc.)
    - ST3: å¤šæ ‡ç­¾è¡¨ç°å½¢å¼è¯†åˆ« (Stereotype, Dehumanization, etc.)
    """

    def __init__(self, data_path, tokenizer_name, max_length=128, task="st1"):
        self.data = []
        self.task = task.lower()
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length

        # --- 1. æ™ºèƒ½è·¯å¾„è§£æ ---
        # æ— è®ºåœ¨ Colab è¿˜æ˜¯æœ¬åœ°ï¼Œè‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•
        try:
            self.project_root = Path(__file__).resolve().parents[2]
        except NameError:
            self.project_root = Path(os.getcwd())

        absolute_path = self.project_root / data_path if not Path(data_path).is_absolute() else Path(data_path)

        # --- 2. æ ¸å¿ƒâ€œé’©å–â€é€»è¾‘ ---
        print(f"ğŸ” æ­£åœ¨ä» {absolute_path} åŠ è½½æ•°æ®...")
        with open(absolute_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line)

                # Subtask 1: äºŒåˆ†ç±» (0/1)
                if self.task == "st1":
                    if item.get("label_st1") != -1:
                        self.data.append({
                            "text": item["text"],
                            "label": item["label_st1"]
                        })

                # Subtask 2: æåŒ–ç»´åº¦ (å¤šæ ‡ç­¾: 5ç»´)
                elif self.task == "st2":
                    # ä»…å½“æ ·æœ¬è¢«æ ‡è®°ä¸ºæåŒ–æ—¶ï¼Œæ‰ç ”ç©¶å…¶ç»´åº¦
                    if item.get("label_st1") == 1:
                        labels = [
                            item["political"], item["racial/ethnic"],
                            item["religious"], item["gender/sexual"], item["other"]
                        ]
                        self.data.append({"text": item["text"], "label": labels})

                # Subtask 3: è¡¨ç°å½¢å¼ (å¤šæ ‡ç­¾: 6ç§)
                elif self.task == "st3":
                    # ä»…å½“æ ·æœ¬è¢«æ ‡è®°ä¸ºæåŒ–æ—¶ï¼Œæ‰ç ”ç©¶å…¶è¡¨ç°å½¢å¼
                    if item.get("label_st1") == 1:
                        labels = [
                            item["stereotype"], item["vilification"], item["dehumanization"],
                            item["extreme_language"], item["lack_of_empathy"], item["invalidation"]
                        ]
                        self.data.append({"text": item["text"], "label": labels})

        print(f"âœ… {self.task.upper()} æ•°æ®åŠ è½½å®Œæˆï¼æ ·æœ¬è§„æ¨¡: {len(self.data)}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # æ–‡æœ¬åˆ†è¯å¤„ç†
        encoding = self.tokenizer(
            item["text"],
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # --- 3. æ ‡ç­¾å¼ é‡è½¬æ¢é€»è¾‘ ---
        if self.task == "st1":
            # ST1 å¿…é¡»æ˜¯ Long ç±»å‹ï¼Œç”¨äº CrossEntropyLoss
            label_tensor = torch.tensor(item["label"], dtype=torch.long)
        else:
            # ST2/ST3 å¤šæ ‡ç­¾ä»»åŠ¡å¿…é¡»æ˜¯ Float ç±»å‹ï¼Œç”¨äº BCEWithLogitsLoss
            label_tensor = torch.tensor(item["label"], dtype=torch.float)

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": label_tensor
        }