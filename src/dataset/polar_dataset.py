import json
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from pathlib import Path


class PolarDataset(Dataset):
    """
    é€šç”¨æåŒ–æ•°æ®é›†ç±»ï¼Œå·²ä¼˜åŒ–ä¸ºæ”¯æŒ ST1 åˆ†ç±»ä»»åŠ¡
    """

    def __init__(self, data_path, tokenizer_name, max_length=128, task="st1"):
        self.data = []
        self.task = task.lower()
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length

        # è·¯å¾„è§£æé€»è¾‘
        self.project_root = Path(__file__).resolve().parents[2]
        absolute_path = self.project_root / data_path if not Path(data_path).is_absolute() else Path(data_path)

        with open(absolute_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line)

                # ST1: æåŒ–åˆ†ç±» (0/1)
                if self.task == "st1" and item.get("label_st1") != -1:
                    self.data.append({"text": item["text"], "label": item["label_st1"]})

                # ST2: æåŒ–ç¨‹åº¦å›å½’
                elif self.task == "st2" and item.get("label_st2"):
                    if item.get("label_st1") == 1:
                        self.data.append({"text": item["text"], "label": item["label_st2"]})

                # ST3: ç»´åº¦è¯†åˆ«
                elif self.task == "st3" and item.get("label_st3"):
                    if item.get("label_st1") == 1:
                        self.data.append({"text": item["text"], "label": item["label_st3"]})

        print(f"âœ… {self.task.upper()} æ•°æ®é›†åŠ è½½å®Œæˆï¼è§„æ¨¡: {len(self.data)}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        encoding = self.tokenizer(item["text"], max_length=self.max_length,
                                  padding="max_length", truncation=True, return_tensors="pt")

        # ğŸš€ æ ¸å¿ƒä¿®æ”¹é€»è¾‘
        if self.task == "st1":
            # 1. ä¿®æ”¹ä¸º Long ç±»å‹ (åˆ†ç±»ä»»åŠ¡å¿…éœ€)
            # 2. ç§»é™¤ unsqueeze(0)ï¼ŒCrossEntropyLoss æœŸæœ›è¾“å…¥å½¢çŠ¶ä¸º (batch_size,)
            label_tensor = torch.tensor(item["label"], dtype=torch.long)
        else:
            # ST2/ST3 ä¿æŒ float ç±»å‹ (å›å½’æˆ–å¤šæ ‡ç­¾ä»»åŠ¡)
            label_tensor = torch.tensor(item["label"], dtype=torch.float)

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": label_tensor
        }