import json
import torch
import os
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from pathlib import Path


class MultitaskPolarDataset(Dataset):
    """
    æåŒ–æ£€æµ‹å¤šä»»åŠ¡æ•°æ®é›†ç±»
    æ”¯æŒç‰¹æ€§ï¼šæ¨ç†æ³¨å…¥ (Reasoning Injection)ã€K-Fold ç´¢å¼•åˆ‡åˆ†ã€æµ‹è¯•æ¨¡å¼
    """

    def __init__(self, data_path, tokenizer_name, max_length=256, task="st1", is_test=False, indices=None):
        self.all_data = []  # åŸå§‹å…¨é‡æ•°æ®
        self.data = []  # æœ€ç»ˆåŠ è½½çš„æ•°æ®
        self.task = task.lower()
        self.is_test = is_test
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length

        # 1. è·¯å¾„è‡ªåŠ¨è§£æ
        current_path = Path(os.getcwd())
        absolute_path = Path(data_path) if Path(data_path).is_absolute() else current_path / data_path

        if not absolute_path.exists():
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {absolute_path}")

        # 2. æ•°æ®åŠ è½½ä¸æ ‡ç­¾è¿‡æ»¤
        with open(absolute_path, 'r', encoding='utf-8') as f:
            raw_list = [json.loads(line) for line in f if line.strip()]

        for item in raw_list:
            if self.is_test:
                self.all_data.append(item)
            else:
                label = self._extract_label(item)
                if label is not None:
                    # ä»…åŠ è½½æœ‰æ ‡ç­¾ä¸”ç¬¦åˆé€»è¾‘çš„æ•°æ®
                    self.all_data.append({"raw_item": item, "label": label})

        # 3. K-Fold ç´¢å¼•åˆ‡åˆ†
        if indices is not None:
            self.data = [self.all_data[i] for i in indices if i < len(self.all_data)]
        else:
            self.data = self.all_data

        print(f"âœ… {self.task.upper()} [{'TEST' if is_test else 'TRAIN'}] åŠ è½½å®Œæˆï¼æ ·æœ¬æ•°: {len(self.data)}")

    def _extract_label(self, item):
        """æ ¸å¿ƒé€»è¾‘ï¼šæ ¹æ®ä¸åŒä»»åŠ¡æå–æ ‡ç­¾"""
        if self.task == "st1":
            l = item.get("label_st1")
            return l if l in [0, 1] else None
        elif self.task == "st2":
            # åªæœ‰æåŒ–çš„æ ·æœ¬æ‰å‚ä¸ ST2/ST3 è®­ç»ƒ
            return item.get("label_st2") if item.get("label_st1") == 1 else None
        elif self.task == "st3":
            return item.get("label_st3") if item.get("label_st1") == 1 else None
        return None

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        raw_item = item if self.is_test else item["raw_item"]

        text = raw_item.get("text", "")
        analysis = raw_item.get("analysis", "")

        # --- ğŸš€ æ¨ç†æ³¨å…¥ (Reasoning Injection) ---
        if analysis and str(analysis).strip():
            sep = self.tokenizer.sep_token
            text = f"{text} {sep} Analysis: {analysis}"

        # æ ¸å¿ƒä¿®æ”¹ï¼šè¿™é‡Œä¸å†å¡«å……ï¼Œåªåšæˆªæ–­
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding=False,  # <--- å…³é”®ï¼šæ”¹ä¸ºåŠ¨æ€å¡«å……çš„åŸºç¡€
            truncation=True,
            return_tensors="pt"
        )

        # æ ‡ç­¾å¼ é‡è½¬æ¢
        if self.is_test:
            label_tensor = torch.tensor(0)
        elif self.task == "st1":
            label_tensor = torch.tensor(item["label"], dtype=torch.long)
        else:
            label_tensor = torch.tensor(item["label"], dtype=torch.float)

        res = {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": label_tensor
        }

        # æ¨ç†æ¨¡å¼è¿”å› ID ç”¨äºç»“æœæ‰“åŒ…
        if self.is_test:
            res["id"] = raw_item.get("id", "unknown")

        return res