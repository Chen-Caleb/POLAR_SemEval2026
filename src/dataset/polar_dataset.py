import json
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from pathlib import Path


class PolarDataset(Dataset):
    """
    é€šç”¨æåŒ–æ•°æ®é›†ç±»ï¼Œé€šè¿‡ task å‚æ•°æ”¯æŒä¸åŒå­ä»»åŠ¡
    """

    def __init__(self, data_path, tokenizer_name, max_length=128, task="st1"):
        self.data = []
        self.task = task.lower()
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length

        # è·¯å¾„è§£æé€»è¾‘ä¿æŒä¸å˜...
        self.project_root = Path(__file__).resolve().parents[2]
        absolute_path = self.project_root / data_path if not Path(data_path).is_absolute() else Path(data_path)

        with open(absolute_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line)

                # ğŸš€ æ ¹æ®ä»»åŠ¡ç±»å‹â€œé’©å‡ºâ€å¯¹åº”æ•°æ®
                if self.task == "st1" and item.get("label_st1") != -1:
                    self.data.append({"text": item["text"], "label": item["label_st1"]})

                elif self.task == "st2" and item.get("label_st2"):
                    # Stage 2 é€»è¾‘ï¼šä»…åŠ è½½å·²æ ‡è®°æåŒ–çš„æ ·æœ¬
                    if item.get("label_st1") == 1:
                        self.data.append({"text": item["text"], "label": item["label_st2"]})

                elif self.task == "st3" and item.get("label_st3"):
                    if item.get("label_st1") == 1:
                        self.data.append({"text": item["text"], "label": item["label_st3"]})

        print(f"âœ… {self.task.upper()} æ•°æ®é›†åŠ è½½å®Œæˆï¼è§„æ¨¡: {len(self.data)}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        encoding = self.tokenizer(item["text"], max_length=self.max_length,
                                  padding="max_length", truncation=True, return_tensors="pt")

        # å°†æ ‡ç­¾è½¬æ¢ä¸ºå¼ é‡ã€‚æ³¨æ„ï¼šST1 æ˜¯å•ä¸ª floatï¼ŒST2/3 æ˜¯åˆ—è¡¨è½¬ float
        label_tensor = torch.tensor(item["label"], dtype=torch.float)
        if self.task == "st1":
            label_tensor = label_tensor.unsqueeze(0)  # ä¿æŒ (1,) å½¢çŠ¶ä¾›äºŒåˆ†ç±»æŸå¤±ä½¿ç”¨

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": label_tensor
        }