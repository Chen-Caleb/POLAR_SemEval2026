import os
import shutil
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from torch.utils.data import Dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer

# ==================== æ ¸å¿ƒé…ç½® ====================
# 1. è§£å‹åçš„æ¨¡å‹è·¯å¾„
PROJECT_ROOT = Path(os.getcwd())

# 2. åŸºäºæ ¹ç›®å½•å®šä¹‰å…¶ä»–å­è·¯å¾„ (ç»Ÿä¸€ä½¿ç”¨ / è¿ç®—ç¬¦ï¼Œä¸è¦æ··ç”¨ os.path.join)
MODEL_PATH = PROJECT_ROOT / "checkpoints" / "st1_baseline"
DEV_DATA_PATH = PROJECT_ROOT / "data" / "processed" / "dev_subtask1.jsonl"

# 3. æäº¤åŒ…ç›¸å…³é…ç½®
SUBMISSION_DIR = "subtask_1"
OUTPUT_ZIP_NAME = "submission_st1"

# ========================================================

class SimplePolarDataset(Dataset):
    def __init__(self, df, tokenizer, max_length=128):
        self.texts = df['text'].astype(str).tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        return {k: v.squeeze(0) for k, v in encoding.items()}


def run_st1_submission():
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ Subtask 1 æäº¤åŒ…...")

    # --- 1. ç¯å¢ƒä¸è·¯å¾„æ ¸å¯¹ ---
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„ {MODEL_PATH}")
        return

    # --- 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ---
    print(f"ğŸ“¦ æ­£åœ¨ä» {MODEL_PATH} åŠ è½½æƒé‡...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)

    # --- 3. åŠ è½½æµ‹è¯•æ•°æ® ---
    if not os.path.exists(DEV_DATA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶ {DEV_DATA_PATH}")
        return
    df = pd.read_json(DEV_DATA_PATH, lines=True)
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(df)} æ¡æµ‹è¯•æ ·æœ¬")

    # --- 4. æ‰§è¡Œé¢„æµ‹ (Classification é€»è¾‘) ---
    print("ğŸ”® æ­£åœ¨è¿›è¡Œæ¨¡å‹æ¨ç† (çœŸåˆ†ç±»æ¨¡å¼)...")
    dataset = SimplePolarDataset(df, tokenizer)
    trainer = Trainer(model=model)

    raw_output = trainer.predict(dataset)
    # æ ¸å¿ƒï¼šä½¿ç”¨ argmax é€‰å–å¾—åˆ†æœ€é«˜çš„ç±»åˆ«ç´¢å¼• (0 æˆ– 1)
    preds = np.argmax(raw_output.predictions, axis=-1)

    df['polarization'] = preds

    # --- 5. æŒ‰è¯­ç§æ‹†åˆ†å¹¶æ‰“åŒ… ---
    print("ğŸ“¦ æ­£åœ¨æŒ‰ç…§ Codabench è§„èŒƒæ ¼å¼åŒ–æ–‡ä»¶...")
    if os.path.exists(SUBMISSION_DIR):
        shutil.rmtree(SUBMISSION_DIR)
    os.makedirs(SUBMISSION_DIR)

    # é€»è¾‘ï¼šä» id ä¸­æå–è¯­è¨€å‰ç¼€ (ä¾‹å¦‚: 'amh_001' -> 'amh')
    df['lang'] = df['id'].apply(lambda x: str(x).split('_')[0])

    for lang in df['lang'].unique():
        lang_df = df[df['lang'] == lang]
        output_file = os.path.join(SUBMISSION_DIR, f"pred_{lang}.csv")
        # ä»…ä¿ç•™æ¯”èµ›è¦æ±‚çš„ id å’Œé¢„æµ‹ç»“æœåˆ—
        lang_df[['id', 'polarization']].to_csv(output_file, index=False)
        print(f"   âœ… å·²ç”Ÿæˆ pred_{lang}.csv")

    # æ‰“åŒ…ä¸º zip
    shutil.make_archive(OUTPUT_ZIP_NAME, 'zip', root_dir=".", base_dir=SUBMISSION_DIR)

    print("\n" + "=" * 50)
    print(f"ğŸ‰ æäº¤åŒ…åˆ¶ä½œå®Œæˆï¼è·¯å¾„: /content/{OUTPUT_ZIP_NAME}.zip")
    print(f"ğŸ’¡ åŒ…å«è¯­ç§: {list(df['lang'].unique())}")
    print("=" * 50)


if __name__ == "__main__":
    run_st1_submission()