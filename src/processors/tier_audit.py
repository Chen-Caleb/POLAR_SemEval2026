import os
import yaml
import torch
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

# ğŸš€ å¯¼å…¥é¡¹ç›®ç»Ÿä¸€çš„ç»„ä»¶
from src.dataset.polar_dataset import MultitaskPolarDataset
from src.engine.evaluator import compute_metrics


def parse_args():
    parser = argparse.ArgumentParser(description="POLAR Tier Audit System")
    parser.add_argument("--config", type=str, default="configs/augmented_st1.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", type=str, required=True, help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (checkpoint)")
    parser.add_argument("--task", type=str, default="st1", help="å®¡è®¡çš„ä»»åŠ¡ (st1/st2/st3)")
    parser.add_argument("--batch_size", type=int, default=64)
    return parser.parse_args()


def run_tier_audit():
    args = parse_args()

    # 1. ç¯å¢ƒä¸é…ç½®åŠ è½½
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ç»Ÿä¸€è¾“å‡ºè·¯å¾„åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ DeepAudit_Results
    output_dir = Path("DeepAudit_Results")
    output_dir.mkdir(exist_ok=True)

    # 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ (ä»æŒ‡å®šçš„ checkpoint åŠ è½½)
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½å—æ£€æ¨¡å‹: {args.checkpoint}")
    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint).to(device)
    model.eval()

    # 3. åŠ è½½æ•°æ®é›† (ä½¿ç”¨ç»Ÿä¸€çš„ MultitaskPolarDataset ä¿è¯æ¨ç†æ³¨å…¥é€»è¾‘ä¸€è‡´)
    dataset = MultitaskPolarDataset(
        data_path=config['data']['train_file'],
        tokenizer_name=args.checkpoint,
        max_length=config['model'].get('max_length', 256),
        task=args.task,
        is_test=False  # éœ€è¦åŠ è½½æ ‡ç­¾è¿›è¡Œå¯¹æ¯”
    )
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)

    results = []
    print(f"ğŸ” å¯åŠ¨äº”å±‚å®¡è®¡ï¼šæ­£åœ¨æ‰«æ {len(dataset)} æ¡æ ·æœ¬...")
    
    # ä½¿ç”¨ enumerate è·Ÿè¸ªå…¨å±€ç´¢å¼•
    global_idx = 0

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader)):
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=mask)
            probs = torch.softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1).cpu().numpy()
            confs = torch.max(probs, dim=1).values.cpu().numpy()

            batch_size = len(labels)
            
            # æ­£ç¡®å¤„ç† batch å†…çš„æ¯ä¸ªæ ·æœ¬
            for i in range(batch_size):
                # è®¡ç®—å…¨å±€ç´¢å¼•
                current_global_idx = global_idx + i
                
                # ä» dataset ä¸­è·å–åŸå§‹æ•°æ®
                if current_global_idx < len(dataset.data):
                    item = dataset.data[current_global_idx]
                    raw_item = item if dataset.is_test else item.get("raw_item", item)
                    
                    # æå– id å’Œ text
                    sample_id = raw_item.get("id", f"unknown_{current_global_idx}")
                    text = raw_item.get("text", "")
                    lang = raw_item.get("lang", str(sample_id).split('_')[0] if '_' in str(sample_id) else "unknown")
                else:
                    # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šå¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´
                    sample_id = f"unknown_{current_global_idx}"
                    text = ""
                    lang = "unknown"
                
                results.append({
                    'id': sample_id,
                    'lang': lang,
                    'text': text,
                    'label': labels[i].item(),
                    'pred': preds[i],
                    'conf': confs[i],
                    'is_correct': labels[i].item() == preds[i]
                })
            
            # æ›´æ–°å…¨å±€ç´¢å¼•
            global_idx += batch_size

    df = pd.DataFrame(results)

    # --- 2. äº”å±‚è¯Šæ–­åˆ†æµé€»è¾‘ (ä¿ç•™åŸå§‹é€»è¾‘) ---
    t1_mask = (~df['is_correct']) & (df['conf'] > 0.90)  # Conflict
    t2_mask = (~df['is_correct']) & (df['conf'] > 0.70) & (df['conf'] <= 0.90)  # Misled
    t3_mask = (~df['is_correct']) & (df['conf'] <= 0.70)  # Confusion
    t4_mask = (df['is_correct']) & (df['conf'] <= 0.70)  # Unstable Corrects

    def save_clean_csv(mask, filename):
        sub_df = df[mask].drop(columns=['is_correct'])
        sub_df.to_csv(output_dir / filename, index=False)
        return len(sub_df)

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†å±‚é”™é¢˜æœ¬è‡³: {output_dir}")
    q1_count = save_clean_csv(t1_mask, f'{args.task}_Conflict_T1.csv')
    save_clean_csv(t2_mask, f'{args.task}_Misled_T2.csv')
    save_clean_csv(t3_mask, f'{args.task}_Confusion_T3.csv')
    save_clean_csv(t4_mask, f'{args.task}_Unstable_Corrects.csv')

    # --- 3. ç”Ÿæˆè¯­ç§å¤šç»´é€è§†æŠ¥å‘Š (ä¿ç•™åŸå§‹é€»è¾‘) ---
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå®¡è®¡åˆ†ææŠ¥å‘Š...")
    report = []
    for lang, group in df.groupby('lang'):
        y_true, y_pred = group['label'], group['pred']
        total = len(group)
        q1 = len(group[(~group['is_correct']) & (group['conf'] > 0.90)])
        q2 = len(group[(~group['is_correct']) & (group['conf'] > 0.70) & (group['conf'] <= 0.90)])
        q3 = len(group[(~group['is_correct']) & (group['conf'] <= 0.70)])
        q4 = len(group[(group['is_correct']) & (group['conf'] <= 0.70)])

        prob_rate = (q1 + q2 + q3 + q4) / total

        report.append({
            'Language': lang,
            'Total': total,
            'Macro_F1': round(f1_score(y_true, y_pred, average='macro', zero_division=0), 4),
            'Accuracy': round(accuracy_score(y_true, y_pred), 4),
            'F1_Binary_P': round(f1_score(y_true, y_pred, average='binary', zero_division=0), 4),
            'Precision_P': round(precision_score(y_true, y_pred, zero_division=0), 4),
            'Recall_P': round(recall_score(y_true, y_pred, zero_division=0), 4),
            'Total_Prob_Rate': f"{round(prob_rate * 100, 2)}%",
            'T1_Conflict_Rate': f"{round(q1 / total * 100, 2)}%",
            'T2_Misled_Rate': f"{round(q2 / total * 100, 2)}%",
            'T3_Confusion_Rate': f"{round(q3 / total * 100, 2)}%",
            'Unstable_Rate': f"{round(q4 / total * 100, 2)}%"
        })

    report_df = pd.DataFrame(report).sort_values(by='Macro_F1')

    # è®¡ç®—å…¨å±€æ±‡æ€»
    g_total = len(df)
    g_q1 = len(df[(~df['is_correct']) & (df['conf'] > 0.90)])
    g_q2 = len(df[(~df['is_correct']) & (df['conf'] > 0.70) & (df['conf'] <= 0.90)])
    g_q3 = len(df[(~df['is_correct']) & (df['conf'] <= 0.70)])
    g_q4 = len(df[(df['is_correct']) & (df['conf'] <= 0.70)])

    avg_row = pd.DataFrame([{
        'Language': 'AVERAGE (GLOBAL)',
        'Total': g_total,
        'Macro_F1': round(f1_score(df['label'], df['pred'], average='macro'), 4),
        'Accuracy': round(accuracy_score(df['label'], df['pred']), 4),
        'F1_Binary_P': round(f1_score(df['label'], df['pred'], average='binary'), 4),
        'Precision_P': round(precision_score(df['label'], df['pred']), 4),
        'Recall_P': round(recall_score(df['label'], df['pred']), 4),
        'Total_Prob_Rate': f"{round((g_q1 + g_q2 + g_q3 + g_q4) / g_total * 100, 2)}%",
        'T1_Conflict_Rate': f"{round(g_q1 / g_total * 100, 2)}%",
        'T2_Misled_Rate': f"{round(g_q2 / g_total * 100, 2)}%",
        'T3_Confusion_Rate': f"{round(g_q3 / g_total * 100, 2)}%",
        'Unstable_Rate': f"{round(g_q4 / g_total * 100, 2)}%"
    }])

    final_report = pd.concat([report_df, avg_row], ignore_index=True)
    final_report.to_csv(output_dir / f'TRAIN_{args.task}_Audit_Report.csv', index=False)

    print("\n" + "=" * 130)
    disp_cols = ['Language', 'Macro_F1', 'F1_Binary_P', 'Total_Prob_Rate', 'T2_Misled_Rate', 'Unstable_Rate']
    print(final_report[disp_cols].to_string(index=False))
    print("=" * 130)
    print(f"ğŸ‰ å®¡è®¡å®Œæˆï¼å…±å‘ç° {q1_count} ä¸ª Tier 1 å†²çªæ ·æœ¬ã€‚ç»“æœå·²å­˜å…¥ {output_dir}")


if __name__ == "__main__":
    run_tier_audit()