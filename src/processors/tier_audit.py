import os
import yaml
import torch
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

# ðŸš€ å¯¼å…¥é¡¹ç›®ç»Ÿä¸€çš„ç»„ä»¶
from src.dataset.polar_dataset import MultitaskPolarDataset
from src.engine.evaluator import compute_metrics


def parse_args():
    parser = argparse.ArgumentParser(description="POLAR Tier Audit System")
    parser.add_argument("--config", type=str, default="configs/augmented_st1.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", type=str, required=True, help="è®­ç»ƒå¥½çš„æ¨¡åž‹è·¯å¾„ (checkpoint)")
    parser.add_argument("--task", type=str, default="st1", help="å®¡è®¡çš„ä»»åŠ¡ (st1/st2/st3)")
    parser.add_argument("--batch_size", type=int, default=64)
    return parser.parse_args()


def run_tier_audit():
    args = parse_args()

    # 1. çŽ¯å¢ƒä¸Žé…ç½®åŠ è½½
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ç»Ÿä¸€è¾“å‡ºè·¯å¾„åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ DeepAudit_Results
    output_dir = Path("DeepAudit_Results")
    output_dir.mkdir(exist_ok=True)

    # 2. åŠ è½½æ¨¡åž‹ä¸Žåˆ†è¯å™¨ (ä»ŽæŒ‡å®šçš„ checkpoint åŠ è½½)
    print(f"ðŸ“¦ æ­£åœ¨åŠ è½½å—æ£€æ¨¡åž‹: {args.checkpoint}")
    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint).to(device)
    model.eval()

    # 3. åŠ è½½æ•°æ®é›† (ä½¿ç”¨ç»Ÿä¸€çš„ MultitaskPolarDataset ä¿è¯æŽ¨ç†æ³¨å…¥é€»è¾‘ä¸€è‡´)
    dataset = MultitaskPolarDataset(
        data_path=config['data']['train_file'],
        tokenizer_name=args.checkpoint,
        max_length=config['model'].get('max_length', 256),
        task=args.task,
        is_test=False  # éœ€è¦åŠ è½½æ ‡ç­¾è¿›è¡Œå¯¹æ¯”
    )
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)

    results = []
    print(f"ðŸ” å¯åŠ¨äº”å±‚å®¡è®¡ï¼šæ­£åœ¨æ‰«æ {len(dataset)} æ¡æ ·æœ¬...")

    with torch.no_grad():
        for batch in tqdm(dataloader):
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=mask)
            probs = torch.softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1).cpu().numpy()
            confs = torch.max(probs, dim=1).values.cpu().numpy()

            for i in range(len(batch['id'])):
                # æ³¨æ„ï¼šä»Ž dataset åŽŸå§‹åˆ—è¡¨èŽ·å– text ä»¥ä¿è¯å¯¹åº”
                results.append({
                    'id': batch['id'][i],
                    'lang': str(batch['id'][i]).split('_')[0],
                    'text': dataset.data[i]['text'],
                    'label': labels[i].item(),
                    'pred': preds[i],
                    'conf': confs[i],
                    'is_correct': labels[i].item() == preds[i]
                })

    df = pd.DataFrame(results)

    # --- 2. äº”å±‚è¯Šæ–­åˆ†æµé€»è¾‘ (ä¿ç•™åŽŸå§‹é€»è¾‘) ---
    t1_mask = (~df['is_correct']) & (df['conf'] > 0.90)  # Conflict
    t2_mask = (~df['is_correct']) & (df['conf'] > 0.70) & (df['conf'] <= 0.90)  # Misled
    t3_mask = (~df['is_correct']) & (df['conf'] <= 0.70)  # Confusion
    t4_mask = (df['is_correct']) & (df['conf'] <= 0.70)  # Unstable Corrects

    def save_clean_csv(mask, filename):
        sub_df = df[mask].drop(columns=['is_correct'])
        sub_df.to_csv(output_dir / filename, index=False)
        return len(sub_df)

    print(f"ðŸ’¾ æ­£åœ¨ä¿å­˜åˆ†å±‚é”™é¢˜æœ¬è‡³: {output_dir}")
    q1_count = save_clean_csv(t1_mask, f'{args.task}_Conflict_T1.csv')
    save_clean_csv(t2_mask, f'{args.task}_Misled_T2.csv')
    save_clean_csv(t3_mask, f'{args.task}_Confusion_T3.csv')
    save_clean_csv(t4_mask, f'{args.task}_Unstable_Corrects.csv')

    # --- 3. ç”Ÿæˆè¯­ç§å¤šç»´é€è§†æŠ¥å‘Š (ä¿ç•™åŽŸå§‹é€»è¾‘) ---
    print("\nðŸ“Š æ­£åœ¨ç”Ÿæˆå®¡è®¡åˆ†æžæŠ¥å‘Š...")
    report = []
    for lang, group in df.groupby('lang'):
        y_true, y_pred = group['label'], group['pred']
        total = len(group)
        q1 = len(group[(~group['is_correct']) & (group['conf'] > 0.90)])
        q2 = len(group[(~group['is_correct']) & (group['conf'] > 0.70) & (group['conf'] <= 0.90)])
        q3 = len(group[(~group['is_correct']) & (group['conf'] <= 0.70)])
        q4 = len(group[(group['is_correct']) & (group['conf'] <= 0.70)])

        prob_rate = (q1 + q2 + q3 + q4) / total

        report.append({
            'Language': lang,
            'Total': total,
            'Macro_F1': round(f1_score(y_true, y_pred, average='macro', zero_division=0), 4),
            'Accuracy': round(accuracy_score(y_true, y_pred), 4),
            'F1_Binary_P': round(f1_score(y_true, y_pred, average='binary', zero_division=0), 4),
            'Precision_P': round(precision_score(y_true, y_pred, zero_division=0), 4),
            'Recall_P': round(recall_score(y_true, y_pred, zero_division=0), 4),
            'Total_Prob_Rate': f"{round(prob_rate * 100, 2)}%",
            'T1_Conflict_Rate': f"{round(q1 / total * 100, 2)}%",
            'T2_Misled_Rate': f"{round(q2 / total * 100, 2)}%",
            'T3_Confusion_Rate': f"{round(q3 / total * 100, 2)}%",
            'Unstable_Rate': f"{round(q4 / total * 100, 2)}%"
        })

    report_df = pd.DataFrame(report).sort_values(by='Macro_F1')

    # è®¡ç®—å…¨å±€æ±‡æ€»
    g_total = len(df)
    g_q1 = len(df[(~df['is_correct']) & (df['conf'] > 0.90)])
    g_q2 = len(df[(~df['is_correct']) & (df['conf'] > 0.70) & (df['conf'] <= 0.90)])
    g_q3 = len(df[(~df['is_correct']) & (df['conf'] <= 0.70)])
    g_q4 = len(df[(df['is_correct']) & (df['conf'] <= 0.70)])

    avg_row = pd.DataFrame([{
        'Language': 'AVERAGE (GLOBAL)',
        'Total': g_total,
        'Macro_F1': round(f1_score(df['label'], df['pred'], average='macro'), 4),
        'Accuracy': round(accuracy_score(df['label'], df['pred']), 4),
        'F1_Binary_P': round(f1_score(df['label'], df['pred'], average='binary'), 4),
        'Precision_P': round(precision_score(df['label'], df['pred']), 4),
        'Recall_P': round(recall_score(df['label'], df['pred']), 4),
        'Total_Prob_Rate': f"{round((g_q1 + g_q2 + g_q3 + g_q4) / g_total * 100, 2)}%",
        'T1_Conflict_Rate': f"{round(g_q1 / g_total * 100, 2)}%",
        'T2_Misled_Rate': f"{round(g_q2 / g_total * 100, 2)}%",
        'T3_Confusion_Rate': f"{round(g_q3 / g_total * 100, 2)}%",
        'Unstable_Rate': f"{round(g_q4 / g_total * 100, 2)}%"
    }])

    final_report = pd.concat([report_df, avg_row], ignore_index=True)
    final_report.to_csv(output_dir / f'TRAIN_{args.task}_Audit_Report.csv', index=False)

    print("\n" + "=" * 130)
    disp_cols = ['Language', 'Macro_F1', 'F1_Binary_P', 'Total_Prob_Rate', 'T2_Misled_Rate', 'Unstable_Rate']
    print(final_report[disp_cols].to_string(index=False))
    print("=" * 130)
    print(f"ðŸŽ‰ å®¡è®¡å®Œæˆï¼å…±å‘çŽ° {q1_count} ä¸ª Tier 1 å†²çªæ ·æœ¬ã€‚ç»“æžœå·²å­˜å…¥ {output_dir}")


if __name__ == "__main__":
    run_tier_audit()