# SemEval-2026 POLAR (Task 9): Multilingual Polarization Detection

## ğŸ“– é¡¹ç›®ç®€ä»‹
æœ¬é¡¹ç›®æ˜¯é’ˆå¯¹ **SemEval-2026 Task 9 (POLAR)** ç«èµ›å¼€å‘çš„å·¥ä¸šçº§è§£å†³æ–¹æ¡ˆã€‚ä»»åŠ¡æ ¸å¿ƒåœ¨äºè¯†åˆ« 22 ç§è¯­è¨€ç¯å¢ƒä¸‹çš„æ”¿æ²»ã€å®—æ•™ã€æ€§åˆ«ç­‰ç»´åº¦çš„æåŒ–ï¼ˆPolarizationï¼‰å€¾å‘ã€‚

æˆ‘ä»¬é‡‡ç”¨äº† **â€œè·¯å¾„ Cï¼šæ··åˆå¢å¼ºæµæ°´çº¿ (Hybrid Augmentation Pipeline)â€**ã€‚è¯¥æ–¹æ³•å·§å¦™åœ°ç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹ (gemini-2.5-flash) çš„æ·±å±‚è¯­ä¹‰ç†è§£èƒ½åŠ›ä¸å°å‹è·¨è¯­è¨€ç¼–ç å™¨ (XLM-RoBERTa) çš„é«˜æ€§èƒ½æ¨ç†ä¼˜åŠ¿ï¼Œå®ç°äº†å‡†ç¡®æ€§ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚

---

## ğŸ› ï¸ æŠ€æœ¯è·¯çº¿ï¼šæ··åˆå¢å¼º (Path C)
æœ¬é¡¹ç›®æ‹’ç»ç›²ç›®å †å æ¨¡å‹å‚æ•°ï¼Œæ ¸å¿ƒæˆ˜æœ¯ä¸º â€œgemini-2.5-flash é©±åŠ¨æ•°æ®ç²¾ç‚¼ï¼ŒXLM-R è´Ÿè´£è½»é‡åŒ–å·¥ä¸šçº§æ¨ç†â€ã€‚

#### æ•°æ®ä¸­å¿ƒåŒ–ç­–ç•¥ï¼š5-Tier è¯Šæ–­æ¶æ„

æˆ‘ä»¬åˆ©ç”¨åŸºåº§æ¨¡å‹å¯¹è®­ç»ƒé›†è¿›è¡Œé¦–è½®æ¨æ–­ï¼ŒåŸºäºç½®ä¿¡åº¦ (Confidence) ä¸ é¢„æµ‹çŠ¶æ€ (Prediction Status) å°†æ•°æ®åˆ’åˆ†ä¸ºäº”ä¸ªå±‚çº§ï¼Œå®æ–½å·®å¼‚åŒ–æ²»ç†ï¼š
å±‚çº§ (Tier),ç°è±¡,è¯Šæ–­é€»è¾‘ (Diagnostic Logic),gemini-2.5-flash å¤„ç†ç­–ç•¥
Tier 1: Conflict,é«˜ç½®ä¿¡åº¦é”™è¯¯,åŒºåˆ† Label Error vs Model Bias,Label Error: ç›´æ¥å‰”é™¤ï¼›Model Bias: ç”Ÿæˆ CoT çº å
Tier 2: Misled,ä¸­é«˜ç½®ä¿¡åº¦é”™è¯¯,å¤æ‚ä¿®è¾è¯¯å¯¼ (åè®½ã€éšå–»),CoT é€»è¾‘æ³¨å…¥: ç”Ÿæˆæ€ç»´é“¾è§£é‡Š
Tier 3: Confusion,ä¸­ä½ç½®ä¿¡åº¦é”™è¯¯,è¯­ä¹‰çœŸç©º (æ¨¡å‹â€œé çŒœâ€),åˆæˆå¢å¼º: åŸºäºæ ·æœ¬ç”Ÿæˆç›¸ä¼¼å˜ä½“
Tier 4: Unstable,ä½ç½®ä¿¡åº¦æ­£ç¡®,å†³ç­–è¾¹ç•Œè„†å¼± (è¿æ°”æˆåˆ†),è·¨è¯­è¨€å¯¹é½: å¼•å…¥è‹±è¯­ç¿»è¯‘ä¸€è‡´æ€§è®­ç»ƒ
Tier 5: Stable,é«˜ç½®ä¿¡åº¦æ­£ç¡®,é€»è¾‘ç¨³å¥æŒæ¡,ç»´æŒ (No Action): ä¿æŒæ•°æ®åˆ†å¸ƒ

##### Tier 1 çš„æ·±åº¦ä»²è£æœºåˆ¶ (Arbiter Mechanism)

å¯¹äº Tier 1 æ•°æ®ï¼Œæˆ‘ä»¬è°ƒç”¨ Gemini 2.5 Fast å……å½“â€œé¦–å¸­ä»²è£å®˜â€ï¼š

Label Error åˆ¤å®šï¼šè‹¥ GenAI è®¤ä¸ºæ ‡æ³¨è¿èƒŒé€»è¾‘ï¼Œæ‰§è¡Œç¡¬å‰”é™¤ (Hard Removal)ï¼Œå‡€åŒ–è®­ç»ƒé›†ã€‚

Model Bias çº åï¼šè‹¥æ ‡æ³¨æ­£ç¡®ä½†æ¨¡å‹è¯¯åˆ¤ï¼Œç”Ÿæˆ Chain-of-Thought (CoT)ã€‚

##### Tier 2çš„ æ¨ç†æ³¨å…¥è®­ç»ƒ (Reasoning-Injected Training)

æˆ‘ä»¬å°† Tier 1 (Bias) å’Œ Tier 2 çš„ CoT ç»“æœæ‹¼æ¥è‡³åŸå§‹æ–‡æœ¬ï¼š 
é€»è¾‘æ³¨å…¥ (Injection): é‡‡ç”¨ æ¨ç†å¢å¼ºè¾“å…¥ (Reasoning-Augmented Input) ç­–ç•¥ã€‚

ä¼ ç»Ÿåšæ³•: Input = Text

æœ¬é¡¹ç›®åšæ³•: Input = [CLS] Text [SEP] Gemini_Rationale [SEP]

è¿™è¿«ä½¿ XLM-RoBERTa åœ¨å¾®è°ƒé˜¶æ®µå­¦ä¹ æ·±å±‚ä¿®è¾é€»è¾‘ï¼Œè€Œéè¡¨å±‚å…³é”®è¯åŒ¹é…ã€‚

**æ³¨æ„åŠ›å›¾è°±æ ¡å‡† (Attention Map Calibration):** åœ¨è®­ç»ƒå¾®è°ƒé˜¶æ®µï¼Œå°†å…³é”®è¯ä¸ CoT é€»è¾‘ä½œä¸ºæ˜¾æ€§ä¿¡å·æ³¨å…¥ã€‚åœ¨ $Self-Attention$ è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œè¿™äº›ä¿¡å·å……å½“äº†**â€œç‰¹å¾é”šç‚¹â€**ï¼Œå¼ºè¿«æ¨¡å‹å°†æ³¨æ„åŠ›æƒé‡ä»åŸå§‹æ–‡æœ¬ä¸­çš„å™ªå£°è¯æ±‡æ±‡èšåˆ°æ ¸å¿ƒæåŒ–è¯­ä¹‰ä¸Šã€‚

##### Tier 3: è¯­ä¹‰çœŸç©ºå¡«è¡¥ (Filling Semantic Void) â€” ç”Ÿæˆæ›´å¤šæ•°æ®

**æ ¸å¿ƒç›®çš„ï¼šå¡«è¡¥â€œè¯­ä¹‰çœŸç©ºâ€ï¼Œé‡å¡‘ç‰¹å¾åˆ†å¸ƒã€‚**

- **ç°è±¡æœ¬è´¨ï¼š** Tier 3 çš„æ ·æœ¬å¤„äºæ¨¡å‹å®Œå…¨æ— æ³•ç†è§£çš„â€œçœŸç©ºåœ°å¸¦â€ã€‚æ­¤æ—¶æ¨¡å‹åœ¨ç‰¹å¾ç©ºé—´ä¸­æ‰¾ä¸åˆ°å¯¹åº”çš„åˆ†ç±»é”šç‚¹ï¼Œå…¶è¾“å‡ºç±»ä¼¼äºéšæœºçŒœæµ‹ã€‚
- **ä½œç”¨é“¾ä¼ å¯¼ï¼š**
  - **é€»è¾‘é©±åŠ¨é‡‡æ · (Logic-Driven Sampling)ï¼š** åˆ©ç”¨ Gemini 2.5 Fast çš„ **æ€ç»´é“¾ (CoT)** å…ˆæ¨å¯¼æåŒ–é€»è¾‘ï¼Œå†ç”Ÿæˆæ–‡æœ¬ã€‚è¿™ä¿è¯äº†æ–°ç”Ÿæˆçš„æ•°æ®ä¸æ˜¯æ— æ„ä¹‰çš„å™ªå£°ï¼Œè€Œæ˜¯å…·æœ‰æé«˜**ç‰¹å¾å¯†åº¦**çš„â€œæ•™ç§‘ä¹¦çº§â€æ ·æœ¬ã€‚
  - **ç‰¹å¾ç©ºé—´æ‰©å¼ ï¼š** æ›´å¤šåŒç±»é€»è¾‘æ ·æœ¬çš„åŠ å…¥ï¼Œå¼ºè¿« XLM-RoBERTa åœ¨åŸæœ¬ç¨€ç–çš„ç‰¹å¾åŒºåŸŸå»ºç«‹èµ·æ¸…æ™°çš„å†³ç­–è¾¹ç•Œã€‚
  - **ç»“æœå½±å“ï¼š** æ¨¡å‹ä¸å†ä¾èµ–ç»Ÿè®¡å·§åˆï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ å¤§é‡é€»è¾‘ä¸€è‡´çš„æ ·æœ¬ï¼Œå»ºç«‹èµ·å¯¹è¯¥ç±»æåŒ–ç°è±¡çš„**å› æœç†è§£**ã€‚

##### Tier 4: å†³ç­–è¾¹ç•Œç¨³å›º (Stabilizing Weak Boundary) â€” è·¨è¯­è¨€åæ ‡å¯¹é½

##### **1. è¯Šæ–­é€»è¾‘ (Diagnostic Logic)**

è¯¥å±‚çº§æ ·æœ¬å±äº**â€œéç¨³å¥æ­£ç¡®â€**ï¼ˆé¢„æµ‹çŠ¶æ€ï¼š**Correct**ï¼Œç½®ä¿¡åº¦ï¼š**ä½**ï¼‰ã€‚æ ·æœ¬è¢«å®šä½åœ¨åˆ†ç±»è¾¹ç•Œçš„ä¸´ç•ŒåŒºåŸŸï¼Œå…¶å‘é‡è¡¨ç¤ºï¼ˆVector Representationï¼‰ææ˜“å—å¾®å°æ‰°åŠ¨ï¼ˆå¦‚æ–¹è¨€å™ªå£°ã€æ‹¼å†™åå·®ï¼‰çš„å½±å“è€Œå‘ç”Ÿè¯­ä¹‰åç§»ï¼Œå¯¼è‡´é¢„æµ‹ç¿»è½¬ã€‚

##### **2. ä½œç”¨æœºåˆ¶ï¼šè¯­ä¹‰åæ ‡è½´æ˜ å°„**

- **å¼ºç‰¹å¾æºé”šå®š (Anchor Reinforcement):** æ¿€æ´» XLM-RoBERTa é¢„è®­ç»ƒç©ºé—´ä¸­åˆ†å¸ƒæœ€ç¨³å¥ã€é€»è¾‘æœ€ä¸¥å¯†çš„â€œè‹±è¯­è¯­ä¹‰åŸŸâ€ï¼Œå°†å…¶ä½œä¸ºå‚ç…§ç³»ã€‚
- **è·¨è¯­è¨€åæ ‡å¯¹é½ (Cross-lingual Coordinate Alignment):** å°†åŸæ–‡ä¸ Gemini ç”Ÿæˆçš„è‹±è¯­ç¿»è¯‘é…å¯¹ã€‚ç¿»è¯‘åœ¨æ­¤æœºåˆ¶ä¸­å……å½“**è™šæ‹Ÿè¯­ä¹‰åæ ‡è½´**ï¼Œå°†æ¨¡ç³Šæˆ–ä½èµ„æºçš„è¾“å…¥ä¿¡å·å¼ºåˆ¶æŠ•å½±è‡³è‹±è¯­æˆç†Ÿçš„é€»è¾‘ç©ºé—´ä¸­ã€‚æ­¤ä¸¾æ—¨åœ¨åˆ©ç”¨æ¨¡å‹çš„â€œæ¯è¯­çº§ä¼˜åŠ¿â€æ¥æ ¡å‡†å¯¹å¤šè¯­è¨€è¾“å…¥çš„**å½’çº³åè§ (Inductive Bias)**ã€‚

##### **3. æ ¸å¿ƒä»·å€¼ (Impact)**

ç¿»è¯‘ä¸ä»…æ˜¯ä¿¡æ¯è¡¥å……ï¼Œæ›´å®ç°äº†**â€œå‘é‡åœºå¯¹é½â€**ä½œç”¨ï¼šå®ƒå°†åŸæœ¬æ¸¸ç¦»åœ¨è¾¹ç•Œå¤„çš„æ ·æœ¬å‘é‡å¼ºåŠ›æ‹‰å›è‡³ç¨³å¥çš„èšç±»ä¸­å¿ƒã€‚é€šè¿‡è¿™ä¸€æ ¡å‡†è¿‡ç¨‹ï¼Œæ¨¡å‹åœ¨ä¸ä¾èµ–å¤–éƒ¨è¾…åŠ©çš„æƒ…å†µä¸‹ï¼Œå…¶å†…éƒ¨å†³ç­–é€»è¾‘çš„ä¸€è‡´æ€§å’Œè·¨è¯­è¨€é²æ£’æ€§å¾—åˆ°äº†æ˜¾è‘—å¢å¼ºã€‚



##### ğŸ§ª æ¶ˆèå®éªŒï¼šè·¨è¯­è¨€å¯¹é½è·¯å¾„çš„æ·±åº¦è¯„æµ‹ (Ablation Study)

é’ˆå¯¹ **Tier 4 (Unstable)** æ ·æœ¬ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¥å¯†çš„æ§åˆ¶å˜é‡å®éªŒï¼Œæ—¨åœ¨æ¢ç©¶è‹±è¯­â€œå¼ºç‰¹å¾æºâ€å¯¹å†³ç­–è¾¹ç•Œç¨³å›ºæ€§çš„æœ€ä¼˜ä¼ å¯¼è·¯å¾„ã€‚

##### æ–¹æ¡ˆè®¾è®¡å¯¹æ¯” (Methodology Comparison)

- **æ–¹æ¡ˆ Aï¼šç›´æ¥ç¿»è¯‘æ‹¼æ¥ (Implicit Concatenation)**
  - **å®ç°**: å°†åŸæ–‡ä¸ç¿»è¯‘ç›´æ¥æ‹¼æ¥ä¸ºå•ä¸€åºåˆ—ï¼š`[åŸæ–‡] [SEP] [ç¿»è¯‘]`ã€‚
  - **åŸç†**: ä¾èµ– Transformer çš„ **Self-Attention** æœºåˆ¶è¿›è¡Œâ€œæ³¨æ„åŠ›å›¾è°±æ ¡å‡† (Attention Map Calibration)â€ï¼Œåœ¨ç¼–ç é˜¶æ®µè®©åŸæ–‡ Token è‡ªåŠ¨è§‚å¯Ÿç¿»è¯‘ Token çš„ç‰¹å¾ã€‚
- **æ–¹æ¡ˆ Bï¼šæ˜¾å¼ä¸€è‡´æ€§æŸå¤± (Explicit $L_{Consistency}$)**
  - **å®ç°**: é‡‡ç”¨åŒæµè¾“å…¥ã€‚åŸæ–‡é€šè¿‡æ¨¡å‹å¾—åˆ°é¢„æµ‹åˆ†å¸ƒ $P_{orig}$ï¼Œç¿»è¯‘é€šè¿‡åŒä¸€æ¨¡å‹å¾—åˆ° $P_{trans}$ã€‚
  - **åŸç†**: å¼•å…¥ **Kullback-Leibler (KL) æ•£åº¦** ä½œä¸ºæ­£åˆ™é¡¹ï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨ä»…è¾“å…¥åŸæ–‡æ—¶ï¼Œå…¶é¢„æµ‹åˆ†å¸ƒä¹Ÿè¦å¯¹é½ç¿»è¯‘è¾“å…¥çš„é¢„æµ‹åˆ†å¸ƒã€‚
  - **å…¬å¼**: $L_{total} = L_{task} + \delta \cdot D_{KL}(P_{orig} || P_{trans})$

##### **å®éªŒç»“è®ºï¼šä¸ºä»€ä¹ˆé€‰æ‹©æ–¹æ¡ˆ Bï¼Ÿ**

å®éªŒè§‚æµ‹æ˜¾ç¤ºï¼Œè™½ç„¶æ–¹æ¡ˆ A åœ¨è®­ç»ƒé›†æ”¶æ•›æ›´å¿«ï¼Œä½†**æ–¹æ¡ˆ B (æ˜¾å¼æŸå¤±)** åœ¨æµ‹è¯•é›†ä¸Šçš„æ³›åŒ–è¡¨ç°æ›´ä¸ºå“è¶Šã€‚å…¶æ ¸å¿ƒå› æœé“¾åœ¨äºï¼š

- **æ–¹æ¡ˆ A** å€¾å‘äºå½¢æˆå¯¹â€œç¿»è¯‘ Tokenâ€çš„**ç‰¹å¾ä¾èµ–**ï¼Œå¯¼è‡´æ¨ç†é˜¶æ®µï¼ˆè„±ç¦»ç¿»è¯‘æ—¶ï¼‰æ€§èƒ½é€€åŒ–ã€‚
- **æ–¹æ¡ˆ B** å®ç°äº†çœŸæ­£çš„**çŸ¥è¯†è’¸é¦**ã€‚é€šè¿‡ Loss é©±åŠ¨ï¼Œè‹±è¯­çš„é€»è¾‘ç¨³å¥æ€§è¢«æ°¸ä¹…å›ºåŒ–åœ¨æ¨¡å‹å¯¹åŸæ–‡å¤„ç†çš„ç¥ç»å…ƒæƒé‡ä¸­ã€‚æ¨¡å‹å­¦ä¼šäº†â€œåœ¨æ²¡æœ‰è„šæ‰‹æ¶çš„æƒ…å†µä¸‹ä¾ç„¶ä¿æŒåæ ‡æ­£ç¡®â€ã€‚





### ğŸ”¥ æ€§èƒ½ä¼˜åŒ–ä¸é²æ£’æ€§å¢å¼º (Performance & Robustness Optimization)

ä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨å¤šè¯­è¨€æåŒ–ä»»åŠ¡ä¸­çš„æ³›åŒ–è¾¹ç•Œï¼Œæœ¬é¡¹ç›®è¿›å…¥äº†åŸºäºâ€œ**ç¨³å®šæ€§ä¼˜å…ˆ**â€çš„æ€§èƒ½å†²åˆºé˜¶æ®µã€‚ç›®å‰æ­£å®æ–½ä»¥ä¸‹å››é¡¹æ ¸å¿ƒç®—æ³•ä¼˜åŒ–ï¼š

##### 1. å¯¹æŠ—è®­ç»ƒ (Adversarial Training) â€” [In Progress]

**æ ¸å¿ƒåŸç†**: å¼•å…¥ **FGM (Fast Gradient Method)** ç®—æ³•ã€‚åœ¨ Embedding å±‚è®¡ç®—æ¢¯åº¦åï¼Œæ²¿ç€æ¢¯åº¦ä¸Šå‡æ–¹å‘æ³¨å…¥å¾®å°çš„å™ªå£°æ‰°åŠ¨ $\delta$ï¼š

$$x_{adv} = x + \epsilon \cdot \frac{g}{||g||_2}$$

**å­¦æœ¯åŠ¨æœº**: è¿™ç§æ–¹å¼æœ¬è´¨ä¸Šæ˜¯åœ¨è¿›è¡Œ**æµå½¢å¹³æ»‘ (Manifold Smoothing)**ã€‚é€šè¿‡â€œå¸¦æ²™è¢‹è®­ç»ƒâ€ï¼Œå¼ºè¿«æ¨¡å‹æ”¾å¼ƒä¾èµ–ä¸ç¨³å®šçš„å±€éƒ¨ç»Ÿè®¡ç‰¹å¾ï¼Œè½¬è€Œæ•è·æ›´å…·æ³›åŒ–æ€§çš„å…¨å±€è¯­ä¹‰ç‰¹å¾ã€‚ **é¢„æœŸæ•ˆåº”**: æ˜¾è‘—å¢å¼ºæ¨¡å‹å¯¹æ‹¼å†™å˜ä½“ã€ç¿»è¯‘å™ªå£°åŠæ”»å‡»æ€§æ–‡æœ¬çš„é²æ£’æ€§ï¼Œé¢„æœŸæå‡ **0.5% - 1.0%** çš„ Macro-F1ã€‚



##### 2. å¤šé‡ Dropout (Multi-Sample Dropout) â€” [In Progress]

**æ ¸å¿ƒåŸç†**: åœ¨åˆ†ç±»å¤´ï¼ˆClassification Headï¼‰éƒ¨ç½² 5 ç»„å¹¶è¡Œçš„ã€å…·æœ‰ä¸åŒéšæœºæ©ç çš„ Dropout å±‚ã€‚åŒä¸€ç»„éšçŠ¶æ€ï¼ˆHidden Statesï¼‰ç»ç”±ä¸åŒé€šè·¯åï¼Œå¯¹å¾—åˆ°çš„ 5 ç»„ Loss è¿›è¡Œè”åˆå‡å€¼ä¼˜åŒ–ã€‚ **å­¦æœ¯åŠ¨æœº**: è¿™æ˜¯ä¸€ç§**é«˜æ•ˆçš„éšæœºæ­£åˆ™åŒ– (Stochastic Regularization)** æ‰‹æ®µã€‚å®ƒåœ¨å•ä¸€æ¨¡å‹å†…éƒ¨å®ç°äº†â€œéšå¼é›†æˆâ€ï¼Œæœ‰æ•ˆç¼“è§£äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨å°æ ·æœ¬ä»»åŠ¡ä¸Šçš„è¿‡æ‹Ÿåˆé£é™©ã€‚ **é¢„æœŸæ•ˆåº”**: åŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼Œå¹¶åœ¨æåŒ–åˆ¤å®šçš„å†³ç­–è¾¹ç•Œä¸Šè·å¾—æ›´å¹³æ»‘çš„æ¦‚ç‡åˆ†å¸ƒã€‚



##### 3. äº¤å‰éªŒè¯é›†æˆ (K-Fold Ensemble) â€” [Planned]

**æ ¸å¿ƒåŸç†**: é‡‡ç”¨ **5-Fold Cross-Validation** ç­–ç•¥æ›¿ä»£ä¼ ç»Ÿçš„ Hold-out éªŒè¯ã€‚ **å·¥ç¨‹ä»·å€¼**:

- **ç»Ÿè®¡ç¨³å®šæ€§**: è®­ç»ƒ 5 ä¸ªç‹¬ç«‹çš„ XLM-R å®ä¾‹ï¼Œç¡®ä¿æ¨¡å‹å¯¹ 22 ç§å¤šè¯­è¨€åˆ†å¸ƒçš„è¦†ç›–ä¸å­˜åœ¨é‡‡æ ·åå·®ã€‚
- **å†³ç­–ä¸€è‡´æ€§**: æ¨ç†é˜¶æ®µé€šè¿‡ **æ¦‚ç‡å¹³å‡ (Probability Averaging)** è¿›è¡Œè½¯é›†æˆã€‚è¿™èƒ½æœ‰æ•ˆæŠµæ¶ˆå•ä¸€æ¨¡å‹åœ¨ä½èµ„æºè¯­ç§ä¸Šçš„æ¨æ–­æ³¢åŠ¨ï¼Œæ˜¯æå‡æ¦œå•æ’åç¨³å®šæ€§çš„â€œå‹èˆ±çŸ³â€ã€‚

### 

##### 4. ä¼ªæ ‡ç­¾è‡ªè®­ç»ƒ (Pseudo-Labeling) â€” [Planned]

æ ¸å¿ƒåŸç†: é‡‡ç”¨ åŠç›‘ç£å­¦ä¹  (Semi-supervised Learning) èŒƒå¼ï¼ŒæŒ–æ˜æœªæ ‡æ³¨æ•°æ®ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚

å·¥ç¨‹é€»è¾‘:

1. åˆ©ç”¨å½“å‰æœ€ä½³æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æ¨æ–­ã€‚

2. ä¸¥æ ¼ç­›é€‰ç½®ä¿¡åº¦é˜ˆå€¼ **$Conf > 0.99$** çš„é«˜å¯é æ ·æœ¬ä½œä¸ºâ€œé“¶æ ‡æ•°æ® (Silver Labels)â€ã€‚

3. å°†ä¼ªæ ‡ç­¾æ ·æœ¬ä¸åŸå§‹è®­ç»ƒé›†æ··åˆï¼Œè¿›è¡ŒäºŒé˜¶æ®µå¢é‡å¾®è°ƒã€‚

   é¢„æœŸæ•ˆåº”: é€šè¿‡è‡ªé€‚åº”åˆ†å¸ƒå¹³æ»‘ï¼Œæ‰©å¤§æ¨¡å‹å¯¹é•¿å°¾æåŒ–ä¿®è¾çš„æ„ŸçŸ¥èŒƒå›´ï¼Œå®ç°æ€§èƒ½çš„äºŒæ¬¡è·ƒè¿ã€‚





#### ğŸ“Š Experimental Results & Analysis

##### Performance vs. Reliability (Path C)
The following visualization demonstrates the effectiveness of our **Augmented Pipeline (Path C)** compared to the baseline.

![Performance vs Reliability](assets/performance_reliability_path_c.png)

##### Key Observations:
* **Metric 1 (F1 Score Enhancement)**: Our Path C configuration (utilizing FGM, Multi-Sample Dropout, and Reasoning Injection) consistently outperforms the baseline across almost all 22 languages.
* **Metric 2 (Reliability Analysis)**: 
    * The **Total Prob Rate (Aggregated Error Rate)** has decreased significantly (indicated by the solid red line vs. the dashed orange line).
    * There is a notable reduction in **Tier 1 (Conflict)** and **Tier 2 (Misled)** samples, proving that Gemini Arbitration and Adversarial Training have effectively smoothed the decision boundaries.
* **Language Specifics**: High-resource languages like `eng`, `rus`, and `deu` show near-perfect F1 scores, while low-resource or complex languages like `khm` and `ita` show the most dramatic relative improvements.











### ğŸ“‚ ç›®å½•ç»“æ„ (Project Structure)

<pre>
POLAR_SemEval2026/
â”œâ”€â”€ configs/                # Experiment configurations (YAML)
â”‚   â”œâ”€â”€ augmented_st1.yaml  # Config for augmented training
â”‚   â”œâ”€â”€ baseline_st1.yaml   # Config for baseline training
â”‚   â””â”€â”€ inference.yaml      # Config for production inference
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ raw/                # Official competition datasets
â”‚   â”œâ”€â”€ augmented/          # Gemini-arbitrated silver labels
â”‚   â””â”€â”€ processed/          # Cleaned and prepared data
â”œâ”€â”€ src/                    # Source code core
â”‚   â”œâ”€â”€ dataset/            # Data loading & dynamic padding
â”‚   â”‚   â”œâ”€â”€ polar_dataset.py       # Core dataset logic
â”‚   â”‚   â””â”€â”€ data_collator.py       # Dynamic padding for speed
â”‚   â”œâ”€â”€ models/             # Architecture definitions
â”‚   â”‚   â”œâ”€â”€ backbone.py            # Customized XLM-R base
â”‚   â”‚   â””â”€â”€ multi_task_head.py     # Multi-sample dropout heads
â”‚   â”œâ”€â”€ engine/             # Execution core
â”‚   â”‚   â”œâ”€â”€ trainer.py             # FGM adversarial training engine
â”‚   â”‚   â””â”€â”€ evaluator.py           # Metrics calculation (Macro-F1)
â”‚   â”œâ”€â”€ processors/         # Data engineering tools
â”‚   â”‚   â”œâ”€â”€ tier_audit.py          # 5-tier diagnostic system
â”‚   â”‚   â”œâ”€â”€ preprocess.py         
â”‚   â”‚   â””â”€â”€ conflict_arbitrator.py # Gemini-powered LLM arbitrator
â”‚   â””â”€â”€ utils/                    # Helper functions & submission tools
â”‚       â””â”€â”€ submission_tools.py 
â”œâ”€â”€ main.py                 # Unified training entry point
â”œâ”€â”€ get_outputs.py          # Unified inference & packaging entry
â”œâ”€â”€ requirements.txt        # Dependency list
â””â”€â”€ .gitignore              # Git exclusion rules
<pre>



#### ä»£ç è¿è¡ŒæŒ‡å—

#####  `main.py`è¿è¡Œå‘½ä»¤ç¤ºä¾‹

ç›´æ¥åœ¨ç»ˆç«¯/å‘½ä»¤è¡Œè¿è¡Œå³å¯ï¼š

- **è¿è¡ŒåŸºç¡€ç‰ˆå®éªŒï¼š**

  Bash

  ```
  python main.py --config configs/baseline_st1.yaml --task st1
  ```

- **è¿è¡Œæ•°æ®å¢å¼ºç‰ˆå®éªŒï¼š**

  Bash

  ```
  python main.py --config configs/augmented_st1.yaml --task st1
  ```



##### get_outputs.pyè¿è¡Œå‘½ä»¤ç¤ºä¾‹

å‡è®¾ä½ çš„æ¨¡å‹ä¿å­˜åœ¨ `checkpoints/st1_baseline/final_model`ï¼Œä½ å¯ä»¥è¿™æ ·è¿è¡Œï¼š

Bash

```
python get_outputs.py \
  --config configs/inference.yaml \
  --checkpoint checkpoints/st1_baseline/final_model \
  --task st1
```



##### tier_audit.pyè¿è¡Œæ–¹å¼ï¼š

ä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œç›´æ¥è¿è¡Œè¿™ä¸ªå®¡è®¡é€»è¾‘ï¼Œæ— éœ€ä¿®æ”¹è„šæœ¬æºç ï¼š

Bash

```
# è®¾ç½® PYTHONPATH ç¡®ä¿èƒ½æ‰¾åˆ° src æ¨¡å—
export PYTHONPATH=$PYTHONPATH:.

# è¿è¡Œå®¡è®¡
python src/processors/tier_audit.py \
    --config configs/augmented_st1.yaml \
    --checkpoint checkpoints/st1_baseline/final_model \
    --task st1
```

### 
