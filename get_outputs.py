import yaml
import torch
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from torch.utils.data import Dataset
from transformers import AutoTokenizer, Trainer, TrainingArguments

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹å’Œæ•°æ®é›†ç±»
from src.models.backbone import XLMRobertaForPolarization
from src.utils.submission_tools import generate_submission_zip


class InferenceDataset(Dataset):
    """ä¸“é—¨ç”¨äºæ¨ç†çš„æ•°æ®é›†ç±»ï¼Œå¤„ç†æ— æ ‡ç­¾çš„æƒ…å†µï¼Œæ”¯æŒæ¨ç†æ³¨å…¥"""

    def __init__(self, df, tokenizer, max_length=256, task="st1"):
        self.texts = df['text'].astype(str).tolist()
        # æ”¯æŒæ¨ç†æ³¨å…¥ï¼šå¦‚æœæ•°æ®ä¸­æœ‰ analysis å­—æ®µï¼Œåˆ™ä½¿ç”¨
        self.analyses = df.get('analysis', [None] * len(self.texts))
        if isinstance(self.analyses, pd.Series):
            self.analyses = self.analyses.tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.task = task.lower()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        analysis = self.analyses[idx] if idx < len(self.analyses) else None
        
        # ğŸš€ æ¨ç†æ³¨å…¥ï¼šä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        if analysis and str(analysis).strip():
            sep = self.tokenizer.sep_token
            text = f"{text} {sep} Analysis: {analysis}"
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding=False,  # ä½¿ç”¨åŠ¨æ€å¡«å……
            max_length=self.max_length,
            return_tensors="pt"
        )
        return {k: v.squeeze(0) for k, v in encoding.items()}


def main():
    parser = argparse.ArgumentParser(description="Inference Script for POLAR SemEval 2026")
    parser.add_argument("--config", type=str, default="configs/augmented_st1.yaml", help="Path to config file")
    parser.add_argument("--checkpoint", type=str, required=True,
                        help="Path to the specific checkpoint (e.g., checkpoints/st1_baseline/final_model)")
    parser.add_argument("--task", type=str, default="st1", help="Task name (st1/st2/st3)")
    args = parser.parse_args()

    # 1. åŠ è½½é…ç½®ä¸è·¯å¾„
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    checkpoint_path = Path(args.checkpoint)

    # 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ï¼ˆä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç±»ï¼‰
    print(f"ğŸ“¦ æ­£åœ¨ä» {checkpoint_path} åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
    
    # ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°
    num_labels = config['model'].get('num_labels', 2)
    use_multi_dropout = config['model'].get('use_multi_dropout', True)
    num_dropout = config['model'].get('num_dropout', 5)
    dropout_prob = config['model'].get('dropout_prob', 0.1)
    
    # ç¡®å®š backbone æ¨¡å‹åç§°ï¼ˆä»é…ç½®æˆ– checkpoint çš„ config.json è¯»å–ï¼‰
    backbone_name = config['model'].get('backbone', 'xlm-roberta-base')
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç±»åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = XLMRobertaForPolarization(
        model_name=backbone_name,  # ä½¿ç”¨é…ç½®ä¸­çš„ backbone åç§°
        num_labels=num_labels,
        task=args.task,
        use_multi_dropout=use_multi_dropout,
        num_dropout=num_dropout,
        dropout_prob=dropout_prob
    )
    
    # åŠ è½½ä¿å­˜çš„æ¨¡å‹æƒé‡
    model_file = checkpoint_path / "pytorch_model.bin"
    if not model_file.exists():
        # å°è¯• safetensors æ ¼å¼
        from safetensors.torch import load_file
        model_file = checkpoint_path / "model.safetensors"
        if model_file.exists():
            state_dict = load_file(str(model_file))
            model.load_state_dict(state_dict, strict=False)
        else:
            raise FileNotFoundError(f"âŒ åœ¨ {checkpoint_path} ä¸­æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ (pytorch_model.bin æˆ– model.safetensors)")
    else:
        state_dict = torch.load(str(model_file), map_location=device)
        model.load_state_dict(state_dict, strict=False)
    
    model.to(device)
    model.eval()

    # 3. åŠ è½½æµ‹è¯•/éªŒè¯æ•°æ® (ä»é…ç½®æ–‡ä»¶çš„ data è·¯å¾„è¯»å–)
    # æ ¹æ®ä»»åŠ¡é€‰æ‹©å¯¹åº”çš„devæ–‡ä»¶
    dev_file_map = {"st1": "dev_subtask1.jsonl", "st2": "dev_subtask2.jsonl", "st3": "dev_subtask3.jsonl"}
    dev_filename = dev_file_map.get(args.task, "dev_subtask1.jsonl")
    
    test_data_path = Path(config['data']['train_file']).parent / dev_filename
    if not test_data_path.exists():
        # å¦‚æœè·¯å¾„ä¸å¯¹ï¼Œå°è¯•å¯»æ‰¾ data/processed/
        test_data_path = Path("data/processed") / dev_filename

    if not test_data_path.exists():
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶: {test_data_path}")

    print(f"ğŸ“Š æ­£åœ¨è¯»å–æ•°æ®: {test_data_path}")
    df = pd.read_json(test_data_path, lines=True)

    # 4. è®¾ç½®æ¨ç†å¼•æ“ (ä½¿ç”¨ Trainer çš„ predict æ¨¡å¼)
    dataset = InferenceDataset(
        df, 
        tokenizer, 
        max_length=config['model'].get('max_length', 256),
        task=args.task
    )
    
    # éœ€è¦åŠ¨æ€å¡«å……ï¼Œå¯¼å…¥ collator
    from src.dataset.data_collator import get_polar_collator
    data_collator = get_polar_collator(tokenizer)

    # åªéœ€è¦æœ€åŸºæœ¬çš„ TrainingArguments æ¥è¿è¡Œ predict
    training_args = TrainingArguments(
        output_dir="./temp_preds",
        per_device_eval_batch_size=config['train'].get('batch_size', 32),
        fp16=torch.cuda.is_available(),
        report_to="none"
    )

    trainer = Trainer(model=model, args=training_args, data_collator=data_collator)

    # 5. æ‰§è¡Œæ¨ç†
    print(f"ğŸ”® æ­£åœ¨ä¸ºä»»åŠ¡ {args.task.upper()} ç”Ÿæˆé¢„æµ‹...")
    raw_output = trainer.predict(dataset)
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹å¤„ç†é¢„æµ‹ç»“æœ
    if args.task == "st1":
        # äºŒåˆ†ç±»ï¼šå– argmax
        preds = np.argmax(raw_output.predictions, axis=-1)
    else:
        # å¤šæ ‡ç­¾ï¼šä½¿ç”¨ sigmoid + é˜ˆå€¼
        probs = 1 / (1 + np.exp(-raw_output.predictions))  # sigmoid
        preds = (probs > 0.5).astype(int)

    # å°†ç»“æœå¡«å› DataFrame
    # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®ä¸åŒçš„åˆ—å
    if args.task == "st1":
        df['polarization'] = preds
    elif args.task == "st2":
        # ST2: 5ä¸ªæ ‡ç­¾åˆ—
        label_cols = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']
        for i, col in enumerate(label_cols):
            df[col] = preds[:, i] if preds.ndim > 1 else preds
    elif args.task == "st3":
        # ST3: 6ä¸ªæ ‡ç­¾åˆ—
        label_cols = ['stereotype', 'vilification', 'dehumanization', 'extreme_language', 'lack_of_empathy', 'invalidation']
        for i, col in enumerate(label_cols):
            df[col] = preds[:, i] if preds.ndim > 1 else preds

    # 6. è°ƒç”¨å·¥å…·å‡½æ•°ç”Ÿæˆæäº¤åŒ…
    # ç»“æœå­˜æ”¾åœ¨ outputs ç›®å½•ä¸‹
    output_zip_name = f"submission_{args.task}_v1"
    generate_submission_zip(
        df=df,
        output_dir=f"temp_{args.task}",
        zip_name=output_zip_name
    )

    print(f"\nâœ… æ¨ç†ä¸æ‰“åŒ…æµç¨‹ç»“æŸï¼")


if __name__ == "__main__":
    main()