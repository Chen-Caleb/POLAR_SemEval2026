import yaml
import torch
import numpy as np
from pathlib import Path
from sklearn.metrics import f1_score, accuracy_score
from transformers import (
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    set_seed, EarlyStoppingCallback
)
from src.dataset.polar_dataset import PolarDataset


def compute_metrics(eval_pred):
    """ğŸš€ ä¿®æ­£ï¼šé€‚é… 2 ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„åˆ†ç±»æŒ‡æ ‡è®¡ç®—"""
    logits, labels = eval_pred

    # ä»¥å‰æ˜¯ Sigmoidï¼Œç°åœ¨æ˜¯å¯»æ‰¾ 2 ä¸ªè¾“å‡ºä¸­å¾—åˆ†æœ€å¤§çš„ç´¢å¼• (0 æˆ– 1)
    # logits å½¢çŠ¶ä» (batch_size, 1) å˜ä¸º (batch_size, 2)
    predictions = np.argmax(logits, axis=-1)

    return {
        "f1_macro": f1_score(labels, predictions, average='macro'),
        "accuracy": accuracy_score(labels, predictions)
    }


def main():
    # 1. ç¯å¢ƒå‡†å¤‡
    project_root = Path(__file__).resolve().parent
    config_path = project_root / "configs" / "baseline_st1.yaml"

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    set_seed(config['train']['seed'])

    # 2. æ„é€ æ•°æ®é›† (ç¡®ä¿ä½ å·²ç»æ›´æ–°äº† PolarDataset.py ä¸­çš„ dtype=torch.long)
    full_dataset = PolarDataset(
        data_path=config['data']['train_file'],
        tokenizer_name=config['model']['backbone'],
        max_length=config['model']['max_length']
    )

    train_size = int((1 - config['data']['val_split']) * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, val_size])

    # 3. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    # ğŸš€ æ ¸å¿ƒä¿®æ”¹ï¼šnum_labels è®¾ä¸º 2ï¼Œè¿™å°†è‡ªåŠ¨å¯ç”¨ CrossEntropyLoss
    model = AutoModelForSequenceClassification.from_pretrained(
        config['model']['backbone'],
        num_labels=2
    )

    # 4. é…ç½®è®­ç»ƒå‚æ•°
    # 4. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        # ğŸš€ æ”¹è¿› 1ï¼šç›´æ¥å­˜å…¥æŒ‚è½½çš„ Google Driveï¼Œæ–­çº¿ä¹Ÿä¸æ€•
        output_dir="/content/drive/MyDrive/POLAR_Checkpoints/st1_baseline",

        # ğŸš€ æ”¹è¿› 2ï¼šåªä¿ç•™æœ€é‡è¦çš„ 2 ä¸ªæ¨¡å‹åŒ…ï¼ˆèŠ‚çœäº‘ç›˜ç©ºé—´ï¼Œé˜²æ­¢æ’‘çˆ†ï¼‰
        save_total_limit=2,

        # ğŸš€ æ”¹è¿› 3ï¼šå¢åŠ ä¿å­˜é¢‘ç‡ï¼ˆå¯é€‰ï¼‰ï¼Œæ¯”å¦‚æ¯ 500 æ­¥å­˜ä¸€æ¬¡
        # save_strategy="steps",
        # save_steps=500,

        num_train_epochs=config['train']['epochs'],
        per_device_train_batch_size=config['train']['batch_size'],
        per_device_eval_batch_size=config['train']['batch_size'],
        learning_rate=float(config['train']['learning_rate']),
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        fp16=torch.cuda.is_available(),
        logging_dir="./logs",
        report_to="none"
    )

    # 5. å¯åŠ¨ Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    print("ğŸš€ å¼•æ“å·²é‡æ–°å¯åŠ¨ï¼Œæ­£åœ¨ä»¥ã€çœŸåˆ†ç±»æ¨¡å¼ã€‘å¾®è°ƒ Subtask 1...")
    trainer.train()

    # 6. æŒä¹…åŒ–å­˜å‚¨
    trainer.save_model(config['train']['output_dir'])
    print(f"âœ… è®­ç»ƒåœ†æ»¡ç»“æŸï¼ä¿®æ­£åçš„æƒé‡å·²ä¿å­˜è‡³ {config['train']['output_dir']}")


if __name__ == "__main__":
    main()